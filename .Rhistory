install.packages('ggplot2')
install.packages('XML')
install.packages('e1071')
devtools::install_github('rstudio/tensorflow')
install.packages("devtools")
devtools::install_github('rstudio/tensorflow')
library(devtools)
library('devtools')
library(devtools)
library(ggplot2)
library(devtools)
devtools::install_github('rstudio/tensorflow')
install.packages("devtools")
installed.packages('curl')
install.packages('git2r')
devtools::install_github("rstudio/tensorflow")
Sys.setenv(TENSORFLOW_PYTHON="/usr/local/bin/python")
library(tensorflow)
library(tensorflow)
sess = tf$Session()
hello <- tf$constant('Hello, TensorFlow!')
sess$run(hello)
x_data <- runif(100, min=0, max=1)
y_data <- x_data * 0.1 + 0.3
W <- tf$Variable(tf$random_uniform(shape(1L), -1.0, 1.0))
b <- tf$Variable(tf$zeros(shape(1L)))
y <- W * x_data + b
loss <- tf$reduce_mean((y - y_data) ^ 2)
optimizer <- tf$train$GradientDescentOptimizer(0.5)
train <- optimizer$minimize(loss)
sess = tf$Session()
sess$run(tf$global_variables_initializer())
for (step in 1:201) {
sess$run(train)
if (step %% 20 == 0)
cat(step, "-", sess$run(W), sess$run(b), "\n")
}
?tf
?tf$constant
??tf$constant
?tensorflow
library(tensorflow)
datasets <- tf$contrib$learn$datasets
mnist <- datasets$mnist$read_data_sets("MNIST-data", one_hot = TRUE)
head(minst)
str(minst)
mnist
datasets
mnist$train$images
str(mnist$train$images)
str(mnist$train$lable)
str(mnist$train$labels)
x <- td$placeholder(tf$float32,  shape(NULL, 784L))
x <- tf$placeholder(tf$float32,  shape(NULL, 784L))
x
W <- tf$Variable(tf$zeros(shape(784L, 10L)))
W
b <- tf$Variable(tf$zeros(shape(10L)))
b
y <- tf$nn$softmax(tf$matmul(x,w) + b)
y <- tf$nn$softmax(tf$matmul(x,W) + b)
y
y_ <- tf$placeholder(tf$float32, shape(NULL, 10L))
#cost function, cross entropy
corss_entropy <- tf$reduce_mean((-tf$reduce_sum(y_ * tf$log(y),
reduction_indices = 1L)))
#cost function, cross entropy
cross_entropy <- tf$reduce_mean((-tf$reduce_sum(y_ * tf$log(y),
reduction_indices = 1L)))
cross_entropy
optimizer <- tf$train$GradientDescentOptimizer(0.5)
optimizer
train_step <- optimizer$minimize(cross_entropy)
train_step
init <- tf$global_variables_initializer()
sess <- tf$Session()
sess$run(init)
batches <- mnist$train$next_batch((100L))
for(i  in 1:1000){
batches <- mnist$train$next_batch((100L))
batch_xs <- batches[[1]]
batch_ys <- batches[[2]]
sess$run(train_step,
feed_dict = dict(x = batch_xs, y_ = batch_ys))
}
correct_prediction <- tf$equal(tf$argmax(y, 1L), tf$argmax(y_, 1L))
correct_prediction
accuracy <- tf$reduce_mean(tf$cast(correct_prediction, tf$float32))
accuracy
sess$run(accuracy, feed_dict=dict(x = mnist$test$images, y_ = mnist$test$labels))
library(tensorflow)
datasetstf$contrib$learn$datasets
datasets <- tf$contrib$learn$datasets
minist <- datasets$mnist$read_data_sets("MNIST-data", one_hot = TRUE)
sess <- tf$InteractiveSession()
source('~/TensorFlowTutorial/mnist.R')
sigmoid <- function(x){
y <- 1/(1 - exp(-x))
return(y)
}
sigmoid(1.5)
sigmoid(0)
sigmoid <- function(x){
y <- 1/(1 + exp(-x))
return(y)
}
sigmoid(1.5)
sigmoid(0.28)
sigmoid <- function(w,x,b){
y <- 1/(1 + exp(-(w*x+b)))
return(y)
}
sigmoid(w = 0.8, x  = 1.0, b = 0.7)
sigmoid <- function(w, x = 1.0, b){
y <- 1/(1 + exp(-(w*x+b)))
return(y)
}
sigmoid(w = 0.8, x =  1.1, b  = 0.7)
sigmoid(w = -0.01, b  = 0.29)
sigmoid(w = 0.6, x  = 1.0, b = 0.9)
installed.packages('Rweibo')
require(Rweibo)
install.packages('Rweibo')
install.packages('ggplot2')
require(Rweibo)
install.packages("Rweibo", repos = "http://R-Forge.R-project.org")
require(Rweibo)
install.packages('RCurl')
install.packages('rjson')
install.packages('XML')
install.packages('Rweibo')
install.packages("Rweibo", repos = "http://R-Forge.R-project.org")
require(Rweibo)
res <- web.search.content("泰囧", page = 10, sleepmean = 10,
sleepsd = 1)$Weibo
egisterApp(app_name = "SNA3", "********", "****************")
res <- web.search.content("泰囧", page = 10, sleepmean = 10,
sleepsd = 1)$Weibo
registerApp(app_name = "SNA3", "********", "****************")
roauth <- createOAuth(app_name = "SNA3", access_name = "rweibo")
registerApp(app_name = "mySpider", "1732086010", "23f25a03a2b52d9b1079b1794af4535a")
listApp("mySpider")
roauth <- createOAuth(app_name = "mySpider", access_name = "rweibo")
registerApp(app_name = "mySpider", "1732086010", "23f25a03a2b52d9b1079b1794af4535a")
roauth <- createOAuth(app_name = "mySpider", access_name = "rweibo")
roauth <- createOAuth(app_name = "mySpider", access_name = "rweibo")
roauth <- createOAuth(app_name = "mySpider", access_name = "rweibo")
33a9c2618071683a9ad316b6d602d726
res <- web.search.content("泰囧", page = 10, sleepmean = 10,
sleepsd = 1)$Weibo
res
library(stringr)
library(ggplot2)
library(Rwordseg)
library(wordcloud2)
library(ropencc)
library(tm)
library(stringr)
library(ggplot2)
library(Rwordseg)
library(wordcloud2)
library(ropencc)
library(tm)
library(htmlwidgets)
library(Matrix)
setwd("~/midnight_food_store_comment_analysis")
##read data##
# comments <- read.csv(file = "comments.csv",
#                      header = TRUE,
#                      stringsAsFactors = FALSE)
##arrange data##
# comments <- comments[, -7]
#
# comments$comment_time <- str_trim(comments$comment_time)
# comments$comment_time <- as.Date(comments$comment_time)
#
# comments$comment <- str_trim(comments$comment)
# t2sConverter <- converter(T2S)
# comments$comment <- t2sConverter[comments$comment]
#
# comments$usefule_num <- as.integer(comments$usefule_num)
#
# comments$ranking <- factor(comments$ranking,
#                            levels = c("力荐", "推荐", "还行", "较差", "很差", ""),
#                            labels = as.character(5: 0))
# save(comments, file =  "comments.RData")
load("comments.RData")
##plot##
p_ranking <- ggplot(comments) +
geom_bar(aes(x = ranking, fill = ranking))
p_ranking
p_date <- ggplot(comments) +
geom_bar(aes(x = comment_time, fill = ranking))
p_date
##clean text##
comment <- gsub("[a-zA-Z]", "", comments$comment)
comment <- gsub("//.", "", comment)
comment <- comment[!is.na(comment)]
comment <- comment[!nchar(comment) < 2]
##import dictionary##
installDict("dictionary/中华美食.scel","food")
installDict("dictionary/本地明星.scel","names")
installDict("dictionary/深夜食堂【官方推荐】.scel","foodstore")
my_dic <- read.csv(file = "dictionary/mydict.csv",
header = TRUE,
stringsAsFactors = FALSE)
my_dic <- str_trim(my_dic[, 1])
insertWords(my_dic)
##word seg##
seg <- segmentCN(comment)
##convert to corpus
comment.seg <- as.vector(seg)
comment.corpus <- Corpus(VectorSource(comment.seg))
##remove stopwords
stopwords <- read.table("dictionary/stopwords.txt",
header = FALSE,
quote = "",
sep = "\n",
stringsAsFactors = FALSE)
stopwords <- stopwords[, 1]
comment.corpus <- tm_map(comment.corpus, removeWords, stopwords)
##convert to TermDOcumentMatrix##
tdm <- TermDocumentMatrix(comment.corpus, control = list(wordLengths = c(2, Inf)))
# save(tdm, file =  "term_document_matrix.RData")
# inspect(tdm[1:10, 1:2])
##convert to matrix, stored as sparse matrix##
mat <- sparseMatrix(i=tdm$i, j=tdm$j, x=tdm$v,
dims=c(tdm$nrow, tdm$ncol))
v <- rowSums(mat)
d <- data.frame(word = tdm$dimnames$Terms, freq = v,
stringsAsFactors = FALSE)
##plot word cloud and save##
graph <- wordcloud2(d, size = 1,
color = "darkblue",
figPath = "深夜食堂.bmp", widgetsize = c(428, 921))
saveWidget(graph,"tmp.html",selfcontained = FALSE)
segmentCN("跪舔会有好下场吗？不会。去年有个公众号造谣说🍃有害，今年再看已经被封号了。
")
segmentCN("岛国各大电视台高度关注东京都议会改选，纷纷专题报道选举形式、分析竞选人政策，然而东京电视台一如既往地走位风骚，介绍了这些谜一样的信息...哈哈哈哈哈哈哈哈哈转自微博")
?installDict
tdm
dim(tdm)
as.matrix(tdm)
library(tmcn)
seg <- segmentCN(comment)
